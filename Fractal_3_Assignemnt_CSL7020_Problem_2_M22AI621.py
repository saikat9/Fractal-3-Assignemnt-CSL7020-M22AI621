# -*- coding: utf-8 -*-
"""Fractal-3 Assignemnt-CSL7020-Problem-2-M22AI621.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FMafrelBpmbE-nFJLdi7HZWmTPvuuDpG

# Import necessary libraries
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import cv2
import os
import time

from google.colab import drive
drive.mount('/content/drive')

"""# Load the image files for training data and visualise"""

def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder,filename),cv2.IMREAD_GRAYSCALE)
        if img is not None:
            images.append(img)
    return images
train_images_0 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/0/") 
train_images_1 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/1/")
train_images_2 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/2/")
train_images_3 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/3/")
train_images_4 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/4/")
train_images_5 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/5/")
train_images_6 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/6/")
train_images_7 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/7/")
train_images_8 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/8/")
train_images_9 =    load_images_from_folder("/content/drive/MyDrive/train_gurumukhi/9/")

train_list = []
for i in train_images_0:
  i = np.append (0 , i.flatten())
  train_list.append(i)

for i in train_images_1:
  i = np.append (1 , i.flatten())
  train_list.append(i)

for i in train_images_2:
  i = np.append (2 , i.flatten())
  train_list.append(i)

for i in train_images_3:
  i = np.append (3 , i.flatten())
  train_list.append(i)

for i in train_images_4:
  i = np.append (4 , i.flatten())
  train_list.append(i)

for i in train_images_5:
  i = np.append (5 , i.flatten())
  train_list.append(i)

for i in train_images_6:
  i = np.append (6 , i.flatten())
  train_list.append(i)

for i in train_images_7:
  i = np.append (7 , i.flatten())
  train_list.append(i)

for i in train_images_8:
  i = np.append (8 , i.flatten())
  train_list.append(i)        

for i in train_images_9:
  i = np.append (9 , i.flatten())
  train_list.append(i)

train_list_new = []

for i in train_list:
  i = ",".join(str(x) for x in i)
  train_list_new.append(i)

train_list = train_list_new

test_images_0 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/0/") 
test_images_1 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/1/")
test_images_2 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/2/")
test_images_3 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/3/")
test_images_4 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/4/")
test_images_5 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/5/")
test_images_6 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/6/")
test_images_7 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/7/")
test_images_8 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/8/")
test_images_9 =    load_images_from_folder("/content/drive/MyDrive/val_gurumukhi/9/")

test_list = []
for i in test_images_0:
  i = np.append (0 , i.flatten())
  test_list.append(i)

for i in test_images_1:
  i = np.append (1 , i.flatten())
  test_list.append(i)

for i in test_images_2:
  i = np.append (2 , i.flatten())
  test_list.append(i)

for i in test_images_3:
  i = np.append (3 , i.flatten())
  test_list.append(i)

for i in test_images_4:
  i = np.append (4 , i.flatten())
  test_list.append(i)

for i in test_images_5:
  i = np.append (5 , i.flatten())
  test_list.append(i)

for i in test_images_6:
  i = np.append (6 , i.flatten())
  test_list.append(i)

for i in test_images_7:
  i = np.append (7 , i.flatten())
  test_list.append(i)

for i in test_images_8:
  i = np.append (8 , i.flatten())
  test_list.append(i)        

for i in test_images_9:
  i = np.append (9 , i.flatten())
  test_list.append(i)

test_list_new = []

for i in test_list:
  i = ",".join(str(x) for x in i)
  test_list_new.append(i)

test_list = test_list_new

train_list[100]

# take the data from a record, rearrange it into a 20*20 array and plot it as an image
all_values = train_list[100].split(',')
image_array = np.asfarray(all_values[1:]).reshape((20,20))
plt.imshow(image_array, cmap='Greys', interpolation='None')

# scale input to range 0.01 to 1.00
scaled_input_train = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
# print(scaled_input)

test_list[100]

# take the data from a record, rearrange it into a 20*20 array and plot it as an image
all_values = test_list[100].split(',')
image_array = np.asfarray(all_values[1:]).reshape((20,20))
plt.imshow(image_array, cmap='Greys', interpolation='None')

# scale input to range 0.01 to 1.00
scaled_input_test = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
# print(scaled_input)

"""# Building Neural Network"""

class DNN:
  def __init__(self, sizes, epochs, lr):
    self.sizes = sizes
    self.epochs = epochs
    self.lr = lr

    # number of nodes in each layer
    input_layer=self.sizes[0]
    hidden_1=self.sizes[1]
    hidden_2=self.sizes[2]
    output_layer=self.sizes[3]

    self.params = {
        'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),
        'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),
        'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)
    }
  def sigmoid(self, x, derivative=False):
      if derivative:
          return (np.exp(-x))/((np.exp(-x)+1)**2)
      return 1/(1 + np.exp(-x))

  def softmax(self, x, derivative=False):
      # Numerically stable with large exponentials
      exps = np.exp(x - x.max())
      if derivative:
          return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))
      return exps / np.sum(exps, axis=0)
  def forward_pass(self, x_train):
      params = self.params

      # input layer activations becomes sample
      params['A0'] = x_train

      # input layer to hidden layer 1
      params['Z1'] = np.dot(params["W1"], params['A0'])
      params['A1'] = self.sigmoid(params['Z1'])

      # hidden layer 1 to hidden layer 2
      params['Z2'] = np.dot(params["W2"], params['A1'])
      params['A2'] = self.sigmoid(params['Z2'])

      # hidden layer 2 to output layer
      params['Z3'] = np.dot(params["W3"], params['A2'])
      params['A3'] = self.softmax(params['Z3'])

      return params['A3']

  def backward_pass(self, y_train, output):
      '''
          This is the backpropagation algorithm, for calculating the updates
          of the neural network's parameters.

          Note: There is a stability issue that causes warnings. This is 
                caused  by the dot and multiply operations on the huge arrays.
                
                RuntimeWarning: invalid value encountered in true_divide
                RuntimeWarning: overflow encountered in exp
                RuntimeWarning: overflow encountered in square
      '''
      params = self.params
      change_w = {}

      # Calculate W3 update
      error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)
      change_w['W3'] = np.outer(error, params['A2'])

      # Calculate W2 update
      error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)
      change_w['W2'] = np.outer(error, params['A1'])

      # Calculate W1 update
      error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)
      change_w['W1'] = np.outer(error, params['A0'])

      return change_w

  def update_network_parameters(self, changes_to_w):
      '''
          Update network parameters according to update rule from
          Stochastic Gradient Descent.

          θ = θ - η * ∇J(x, y), 
              theta θ:            a network parameter (e.g. a weight w)
              eta η:              the learning rate
              gradient ∇J(x, y):  the gradient of the objective function,
                                  i.e. the change for a specific theta θ
      '''
      
      for key, value in changes_to_w.items():
          self.params[key] -= self.lr * value

  def compute_accuracy(self, test_data, output_nodes):
      '''
          This function does a forward pass of x, then checks if the indices
          of the maximum value in the output equals the indices in the label
          y. Then it sums over each prediction and calculates the accuracy.
      '''
      predictions = []

      for x in test_data:
          all_values = x.split(',')
          # scale and shift the inputs
          inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
          # create the target output values (all 0.01, except the desired label which is 0.99)
          targets = np.zeros(output_nodes) + 0.01
          # all_values[0] is the target label for this record
          targets[int(all_values[0])] = 0.99
          output = self.forward_pass(inputs)
          pred = np.argmax(output)
          predictions.append(pred == np.argmax(targets))
      
      return np.mean(predictions)
     

  def train(self, train_list, test_list, output_nodes):
      start_time = time.time()
      for iteration in range(self.epochs):
          for x in train_list:
              all_values = x.split(',')
              # scale and shift the inputs
              inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
              # create the target output values (all 0.01, except the desired label which is 0.99)
              targets = np.zeros(output_nodes) + 0.01
              # all_values[0] is the target label for this record
              targets[int(all_values[0])] = 0.99
              output = self.forward_pass(inputs)
              changes_to_w = self.backward_pass(targets, output)
              self.update_network_parameters(changes_to_w)
          
          accuracy = self.compute_accuracy(test_list, output_nodes)
          if iteration == 1999:
            iteration = iteration + 1
          if iteration%200 == 0:
            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(
                #iteration+1, time.time() - start_time, accuracy * 100
                iteration, time.time() - start_time, accuracy * 100
            ))

dnn = DNN(sizes=[400, 128, 64, 10], epochs=2000, lr=0.01)
dnn.train(train_list, test_list, 10)